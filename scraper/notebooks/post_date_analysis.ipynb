{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Date Distribution Analysis\n",
    "\n",
    "This notebook analyzes the distribution of posts across dates, identifying dates with and without posts,\n",
    "and calculating post frequency statistics. It generates visualizations and reports to help understand\n",
    "posting patterns over time.\n",
    "\n",
    "## Features\n",
    "- Analyzes post distribution across dates for individual sources:\n",
    "  - Identifies gaps in posting (dates without posts)\n",
    "  - Calculates daily post frequency statistics\n",
    "  - Analyzes posts with significant discussions (configurable reply threshold)\n",
    "  - Supports thread identification by URL or title\n",
    "- Generates cross-platform comparisons:\n",
    "  - Aggregates post frequencies across all sources\n",
    "  - Creates unified daily activity heatmaps\n",
    "- Creates detailed visualizations including:\n",
    "  - Timeline of posts per source\n",
    "  - Post frequency heatmaps\n",
    "  - Gap analysis visualization\n",
    "  - Combined source activity heatmaps\n",
    "- Outputs organized in timestamped directories:\n",
    "  - `/outputs/[source]_[timestamp]/images/` - All visualizations\n",
    "  - `/outputs/[source]_[timestamp]/data/` - CSV data files\n",
    "  - `/outputs/[source]_[timestamp]/reports/` - Markdown analysis reports\n",
    "  - `/outputs/source_comparisons/` - Cross-source analysis visualizations\n",
    "\n",
    "## Requirements\n",
    "- Elasticsearch connection (configured via environment variables)\n",
    "- Python packages: pandas, matplotlib, seaborn, calendar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scraper.config import settings\n",
    "from scraper.outputs import ElasticsearchOutput\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "def setup_output_directories(slug):\n",
    "    \"\"\"\n",
    "    Create nested output directory structure for a given analysis slug.\n",
    "    Returns a dictionary of paths for different output types.\n",
    "    \"\"\"\n",
    "    # Create base outputs directory\n",
    "    base_dir = Path('outputs')\n",
    "    base_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create analysis-specific directory with timestamp\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    analysis_dir = base_dir / f\"{slug}_{timestamp}\"\n",
    "    analysis_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create subdirectories for different output types\n",
    "    images_dir = analysis_dir / 'images'\n",
    "    images_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    data_dir = analysis_dir / 'data'\n",
    "    data_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    reports_dir = analysis_dir / 'reports'\n",
    "    reports_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    return {\n",
    "        'base': analysis_dir,\n",
    "        'images': images_dir,\n",
    "        'data': data_dir,\n",
    "        'reports': reports_dir\n",
    "    }\n",
    "\n",
    "async def get_thread_sizes(es_output, domain, thread_identifier='thread_url'):\n",
    "    \"\"\"\n",
    "    Get the number of replies for each thread in the domain.\n",
    "    \n",
    "    Args:\n",
    "        es_output: ElasticsearchOutput instance\n",
    "        domain: Domain to analyze\n",
    "        thread_identifier: Either 'thread_url' or 'title' to determine how threads are identified\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping thread identifier to reply count\n",
    "    \"\"\"\n",
    "    if thread_identifier not in ['thread_url', 'title']:\n",
    "        raise ValueError(\"thread_identifier must be either 'thread_url' or 'title'\")\n",
    "    \n",
    "    field = f\"{thread_identifier}.keyword\"\n",
    "    \n",
    "    # Build the query based on thread identification method\n",
    "    must_conditions = [{\"term\": {\"domain.keyword\": domain}}]\n",
    "    if thread_identifier == 'thread_url':\n",
    "        must_conditions.append({\"exists\": {\"field\": \"thread_url\"}})\n",
    "    else:\n",
    "        must_conditions.append({\"exists\": {\"field\": \"title\"}})\n",
    "    \n",
    "    query = {\n",
    "        \"size\": 0,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": must_conditions\n",
    "            }\n",
    "        },\n",
    "        \"aggs\": {\n",
    "            \"threads\": {\n",
    "                \"terms\": {\n",
    "                    \"field\": field,\n",
    "                    \"size\": 10000\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results = es_output.es.search(index=settings.DEFAULT_INDEX, body=query)\n",
    "    thread_sizes = {\n",
    "        bucket['key']: bucket['doc_count'] - 1  # Subtract 1 to exclude the original post\n",
    "        for bucket in results['aggregations']['threads']['buckets']\n",
    "    }\n",
    "    \n",
    "    return thread_sizes\n",
    "\n",
    "async def fetch_post_dates(domain, min_replies=None, thread_identifier='thread_url'):\n",
    "    \"\"\"\n",
    "    Fetch posts from Elasticsearch and extract their dates.\n",
    "    \n",
    "    Args:\n",
    "        domain: The domain to fetch posts from\n",
    "        min_replies: Optional minimum number of replies to filter threads\n",
    "        thread_identifier: Either 'thread_url' or 'title' to determine how threads are identified\n",
    "    \"\"\"\n",
    "    es_output = ElasticsearchOutput()\n",
    "    await es_output._initialize()\n",
    "\n",
    "    # Get thread sizes if filtering by replies\n",
    "    thread_sizes = None\n",
    "    if min_replies is not None:\n",
    "        thread_sizes = await get_thread_sizes(es_output, domain, thread_identifier)\n",
    "        active_threads = [thread_id for thread_id, count in thread_sizes.items() if count >= min_replies]\n",
    "        \n",
    "        # Build query based on thread identification method\n",
    "        if thread_identifier == 'thread_url':\n",
    "            thread_filter = {\"terms\": {\"url.keyword\": active_threads}}\n",
    "        else:  # title\n",
    "            thread_filter = {\"terms\": {\"title.keyword\": active_threads}}\n",
    "        \n",
    "        query = {\n",
    "            \"query\": {\n",
    "                \"bool\": {\n",
    "                    \"must\": [\n",
    "                        {\"term\": {\"type.keyword\": \"original_post\"}},\n",
    "                        {\"term\": {\"domain.keyword\": domain}},\n",
    "                        {\"exists\": {\"field\": \"created_at\"}},\n",
    "                        thread_filter\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            \"size\": 10000,\n",
    "            \"sort\": [{\"created_at\": \"asc\"}]\n",
    "        }\n",
    "    else:\n",
    "        query = {\n",
    "            \"query\": {\n",
    "                \"bool\": {\n",
    "                    \"must\": [\n",
    "                        {\"term\": {\"type.keyword\": \"original_post\"}},\n",
    "                        {\"term\": {\"domain.keyword\": domain}},\n",
    "                        {\"exists\": {\"field\": \"created_at\"}}\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            \"size\": 10000,\n",
    "            \"sort\": [{\"created_at\": \"asc\"}]\n",
    "        }\n",
    "\n",
    "    results = es_output.es.search(index=settings.DEFAULT_INDEX, body=query)\n",
    "\n",
    "    data = []\n",
    "    for hit in results['hits']['hits']:\n",
    "        doc = hit['_source']\n",
    "        created_at = datetime.fromisoformat(doc['created_at'].replace('Z', '+00:00'))\n",
    "        data.append({\n",
    "            'id': hit['_id'],\n",
    "            'created_at': created_at.date(),\n",
    "            'url': doc.get('url', 'unknown'),\n",
    "            'title': doc.get('title', 'unknown'),\n",
    "            'author': doc.get('authors', ['unknown'])[0] if doc.get('authors') else 'unknown'\n",
    "        })\n",
    "\n",
    "    await es_output._cleanup()\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def analyze_date_distribution(df, min_replies=None):\n",
    "    \"\"\"\n",
    "    Analyze the distribution of posts across dates.\n",
    "    \"\"\"\n",
    "    # Get date range\n",
    "    date_range = {\n",
    "        'start': df['created_at'].min(),\n",
    "        'end': df['created_at'].max(),\n",
    "        'total_days': (df['created_at'].max() - df['created_at'].min()).days + 1\n",
    "    }\n",
    "    \n",
    "    # Calculate posts per date\n",
    "    posts_per_date = df.groupby('created_at').size()\n",
    "    \n",
    "    # Generate complete date range and identify gaps\n",
    "    all_dates = pd.date_range(date_range['start'], date_range['end'], freq='D')\n",
    "    date_status = pd.Series(0, index=all_dates)\n",
    "    date_status[posts_per_date.index] = posts_per_date\n",
    "    \n",
    "    # Calculate statistics\n",
    "    stats = {\n",
    "        'total_posts': len(df),\n",
    "        'total_days': date_range['total_days'],\n",
    "        'days_with_posts': len(posts_per_date),\n",
    "        'days_without_posts': date_range['total_days'] - len(posts_per_date),\n",
    "        'max_posts_per_day': posts_per_date.max(),\n",
    "        'avg_posts_per_day': len(df) / date_range['total_days'],\n",
    "        'avg_posts_on_active_days': len(df) / len(posts_per_date)\n",
    "    }\n",
    "    \n",
    "    return date_range, posts_per_date, date_status, stats\n",
    "\n",
    "def generate_timeline_plot(date_status, output_path):\n",
    "    \"\"\"\n",
    "    Generate a timeline plot showing posts per day.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Plot the data\n",
    "    plt.bar(date_status.index, date_status.values, alpha=0.6)\n",
    "    \n",
    "    # Customize the plot\n",
    "    title_suffix = f\" (Posts with {min_replies}+ replies)\" if min_replies else \"\"\n",
    "    plt.title(f'Posts per Day Timeline - {source.upper()}{title_suffix}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Number of Posts')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Rotate x-axis labels for better readability\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def generate_heatmap(date_status, output_path):\n",
    "    \"\"\"\n",
    "    Generate a calendar heatmap of posting activity.\n",
    "    \"\"\"\n",
    "    # Prepare data for heatmap\n",
    "    data = []\n",
    "    for date, count in date_status.items():\n",
    "        data.append({\n",
    "            'year': date.year,\n",
    "            'month': date.month,\n",
    "            'day': date.day,\n",
    "            'count': count\n",
    "        })\n",
    "    \n",
    "    df_heatmap = pd.DataFrame(data)\n",
    "    \n",
    "    # Create pivot table for heatmap\n",
    "    pivot_table = df_heatmap.pivot_table(\n",
    "        values='count',\n",
    "        index=['year', 'month'],\n",
    "        columns='day',\n",
    "        aggfunc='sum'\n",
    "    )\n",
    "    \n",
    "    # Generate heatmap\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    # Determine whether to show annotations based on the size of the data\n",
    "    n_rows, n_cols = pivot_table.shape\n",
    "    total_cells = n_rows * n_cols\n",
    "    \n",
    "    # Calculate approximate cell size in inches\n",
    "    fig_size_inches = (15, 8)  # from figsize parameter\n",
    "    cell_height = fig_size_inches[1] / n_rows\n",
    "    cell_width = fig_size_inches[0] / n_cols\n",
    "    cell_size = min(cell_height, cell_width)\n",
    "    \n",
    "    # Show annotations only if cells are large enough (threshold can be adjusted)\n",
    "    show_annot = cell_size >= 0.3\n",
    "    \n",
    "    sns.heatmap(\n",
    "        pivot_table,\n",
    "        cmap='YlOrRd',\n",
    "        annot=show_annot,\n",
    "        fmt='.0f',\n",
    "        cbar_kws={'label': 'Number of Posts'},\n",
    "        # Add these parameters for better readability when annotations are shown\n",
    "        annot_kws={'size': 8} if show_annot else {}\n",
    "    )\n",
    "    \n",
    "    title_suffix = f\" (Posts with {min_replies}+ replies)\" if min_replies else \"\"\n",
    "    plt.title(f'Post Frequency Calendar Heatmap - {source.upper()}{title_suffix}')\n",
    "    plt.xlabel('Day of Month')\n",
    "    plt.ylabel('Year-Month')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def generate_gap_analysis_plot(date_status, output_path):\n",
    "    \"\"\"\n",
    "    Generate a visualization of posting gaps.\n",
    "    \"\"\"\n",
    "    # Find sequences of days without posts\n",
    "    gaps = []\n",
    "    current_gap = 0\n",
    "    current_gap_start = None\n",
    "    \n",
    "    for date, count in date_status.items():\n",
    "        if count == 0:\n",
    "            if current_gap_start is None:\n",
    "                current_gap_start = date\n",
    "            current_gap += 1\n",
    "        else:\n",
    "            if current_gap > 0:\n",
    "                gaps.append({\n",
    "                    'start': current_gap_start,\n",
    "                    'length': current_gap\n",
    "                })\n",
    "            current_gap = 0\n",
    "            current_gap_start = None\n",
    "    \n",
    "    # Add the last gap if exists\n",
    "    if current_gap > 0:\n",
    "        gaps.append({\n",
    "            'start': current_gap_start,\n",
    "            'length': current_gap\n",
    "        })\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    for gap in gaps:\n",
    "        plt.bar(gap['start'], gap['length'], width=1, alpha=0.6)\n",
    "    \n",
    "    title_suffix = f\" (Posts with {min_replies}+ replies)\" if min_replies else \"\"\n",
    "    plt.title(f'Posting Gaps Analysis - {source.upper()}{title_suffix}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Gap Length (Days)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def generate_markdown_report(slug, date_range, stats, df, output_dirs, min_replies=None):\n",
    "    \"\"\"\n",
    "    Generate a markdown report with analysis results.\n",
    "    \"\"\"\n",
    "    filter_note = f\" (Posts with {min_replies}+ replies)\" if min_replies else \"\"\n",
    "    markdown = f\"# Post Date Analysis for {slug}{filter_note}\\n\\n\"\n",
    "    \n",
    "    markdown += \"## Analysis Period\\n\\n\"\n",
    "    markdown += f\"- Start Date: {date_range['start']}\\n\"\n",
    "    markdown += f\"- End Date: {date_range['end']}\\n\"\n",
    "    markdown += f\"- Total Days: {date_range['total_days']}\\n\\n\"\n",
    "    \n",
    "    markdown += \"## Posting Statistics\\n\\n\"\n",
    "    markdown += f\"- Total Posts: {stats['total_posts']}\\n\"\n",
    "    markdown += f\"- Days with Posts: {stats['days_with_posts']}\\n\"\n",
    "    markdown += f\"- Days without Posts: {stats['days_without_posts']}\\n\"\n",
    "    markdown += f\"- Maximum Posts in a Day: {stats['max_posts_per_day']}\\n\"\n",
    "    markdown += f\"- Average Posts per Day: {stats['avg_posts_per_day']:.2f}\\n\"\n",
    "    markdown += f\"- Average Posts on Active Days: {stats['avg_posts_on_active_days']:.2f}\\n\\n\"\n",
    "    \n",
    "    # Add visualizations\n",
    "    markdown += \"## Visualizations\\n\\n\"\n",
    "    markdown += \"### Posts Timeline\\n\"\n",
    "    markdown += \"![Timeline Plot](../images/timeline_plot.png)\\n\\n\"\n",
    "    markdown += \"### Post Frequency Heatmap\\n\"\n",
    "    markdown += \"![Heatmap Plot](../images/heatmap_plot.png)\\n\\n\"\n",
    "    markdown += \"### Posting Gaps Analysis\\n\"\n",
    "    markdown += \"![Gap Analysis Plot](../images/gap_analysis_plot.png)\\n\\n\"\n",
    "    \n",
    "    return markdown\n",
    "\n",
    "def generate_csv(df):\n",
    "    \"\"\"\n",
    "    Generate CSV output with post date information.\n",
    "    \"\"\"\n",
    "    # Sort DataFrame by date\n",
    "    df_sorted = df.sort_values(['created_at', 'id'])\n",
    "    \n",
    "    # Generate CSV\n",
    "    return df_sorted.to_csv(index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = {\n",
    "    # \"delvingbitcoin\": \"https://delvingbitcoin.org/\",\n",
    "    \"bitcoindev\": \"https://mailing-list.bitcoindevs.xyz/bitcoindev/\",\n",
    "    # \"bitcoindev-legacy\": \"https://lists.linuxfoundation.org/pipermail/bitcoin-dev/\",\n",
    "    # \"lightning-dev\": \"https://lists.linuxfoundation.org/pipermail/lightning-dev/\",\n",
    "}\n",
    "\n",
    "# Set minimum replies filter (set to None to see all posts)\n",
    "min_replies = None  # Change this value to adjust the minimum replies threshold\n",
    "thread_identifier = 'title'  # Use either 'thread_url' or 'title'\n",
    "\n",
    "print(f\"index: {settings.DEFAULT_INDEX}\")\n",
    "print(f\"minimum replies filter: {min_replies}\")\n",
    "print(\"\\nStarting analysis for all sources...\")\n",
    "\n",
    "# Process each source\n",
    "for source, domain in domains.items():\n",
    "    try:\n",
    "        print(f\"\\nProcessing {source}...\")\n",
    "        print(f\"domain: {domain}\")\n",
    "        \n",
    "        # Setup output directories\n",
    "        output_dirs = setup_output_directories(source)\n",
    "        \n",
    "        # Fetch and analyze data\n",
    "        df = await fetch_post_dates(domain, min_replies, thread_identifier)\n",
    "        \n",
    "        if df.empty:\n",
    "            print(f\"No data found for {source}, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        date_range, posts_per_date, date_status, stats = analyze_date_distribution(df, min_replies)\n",
    "\n",
    "        # Adjust title suffix based on configuration\n",
    "        title_filter = f\"{min_replies}+ replies (by {thread_identifier})\" if min_replies else \"\"\n",
    "        title_suffix = f\" (Posts with {title_filter})\" if title_filter else \"\"\n",
    "        \n",
    "        # Update titles with filter information if applied\n",
    "        title_suffix = f\" (Posts with {min_replies}+ replies)\" if min_replies else \"\"\n",
    "        \n",
    "        # Generate visualizations\n",
    "        generate_timeline_plot(date_status, output_dirs['images'] / 'timeline_plot.png')\n",
    "        generate_heatmap(date_status, output_dirs['images'] / 'heatmap_plot.png')\n",
    "        generate_gap_analysis_plot(date_status, output_dirs['images'] / 'gap_analysis_plot.png')\n",
    "        \n",
    "        # Generate and save markdown report\n",
    "        markdown_report = generate_markdown_report(source, date_range, stats, df, output_dirs, min_replies)\n",
    "        with open(output_dirs['reports'] / f\"{source}_analysis.md\", \"w\") as f:\n",
    "            f.write(markdown_report)\n",
    "        \n",
    "        # Generate and save CSV\n",
    "        csv_output = generate_csv(df)\n",
    "        with open(output_dirs['data'] / f\"{source}_date_analysis.csv\", \"w\") as f:\n",
    "            f.write(csv_output)\n",
    "        \n",
    "        print(f\"Analysis complete for {source}. Files saved in {output_dirs['base']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {source}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\nAll sources processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Source Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def load_source_data(csv_paths):\n",
    "    \"\"\"\n",
    "    Load data from multiple CSV files and combine them for comparison.\n",
    "    \n",
    "    Args:\n",
    "        csv_paths: Dict mapping source names to CSV file paths\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with data from all sources\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    for source, path in csv_paths.items():\n",
    "        df = pd.read_csv(path)\n",
    "        df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "        df['source'] = source\n",
    "        all_data.append(df)\n",
    "    \n",
    "    return pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "def generate_aggregate_daily_heatmap(combined_df, output_path):\n",
    "    \"\"\"\n",
    "    Generate a heatmap showing total post frequency across all sources per day.\n",
    "    \"\"\"\n",
    "    # Convert to datetime if not already\n",
    "    combined_df['created_at'] = pd.to_datetime(combined_df['created_at'])\n",
    "    \n",
    "    # Create separate columns for year, month, day\n",
    "    daily_counts = combined_df.copy()\n",
    "    daily_counts['year'] = daily_counts['created_at'].dt.year\n",
    "    daily_counts['month'] = daily_counts['created_at'].dt.month\n",
    "    daily_counts['day'] = daily_counts['created_at'].dt.day\n",
    "    \n",
    "    # Group by date components and count\n",
    "    daily_counts = daily_counts.groupby(['year', 'month', 'day']).size().reset_index(name='count')\n",
    "    \n",
    "    # Create pivot table for heatmap\n",
    "    pivot_table = daily_counts.pivot_table(\n",
    "        values='count',\n",
    "        index=['year', 'month'],\n",
    "        columns='day',\n",
    "        fill_value=0\n",
    "    )\n",
    "    \n",
    "    # Rename index for better display\n",
    "    pivot_table.index = [f\"{y}-{m:02d}\" for y, m in pivot_table.index]\n",
    "    \n",
    "    # Set up the plot\n",
    "    plt.figure(figsize=(20, len(pivot_table.index) * 0.4))\n",
    "    \n",
    "    # Create heatmap\n",
    "    n_rows, n_cols = pivot_table.shape\n",
    "    cell_height = 20 / n_cols\n",
    "    show_annot = cell_height >= 0.3\n",
    "    \n",
    "    sns.heatmap(\n",
    "        pivot_table,\n",
    "        cmap='YlOrRd',\n",
    "        annot=show_annot,\n",
    "        fmt='.0f',\n",
    "        cbar_kws={'label': 'Total Posts Across All Sources'},\n",
    "        annot_kws={'size': 6} if show_annot else {}\n",
    "    )\n",
    "    \n",
    "    plt.title('Daily Post Frequency (Aggregated Across All Sources)')\n",
    "    plt.xlabel('Day of Month')\n",
    "    plt.ylabel('Year-Month')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage for multi-source comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "# Define paths to your CSV files\n",
    "csv_paths = {\n",
    "    'BITCOIN-DEV-LEGACY': 'outputs/bitcoindev_YYYYMMDD_HHMMSS/data/bitcoindev-legacy_date_analysis.csv',\n",
    "    'BITCOIN-DEV': 'outputs/bitcoindev_YYYYMMDD_HHMMSS/data/bitcoindev_date_analysis.csv',\n",
    "    'LIGHTNING-DEV': 'outputs/lightning-dev_YYYYMMDD_HHMMSS/data/lightning-dev_date_analysis.csv',\n",
    "    'DELVING-BITCOIN': 'outputs/delvingbitcoin_YYYYMMDD_HHMMSS/data/delvingbitcoin_date_analysis.csv'\n",
    "}\n",
    "\n",
    "# Update these paths with your actual CSV file locations\n",
    "# You can use glob to find the most recent files:\n",
    "\n",
    "base_dir = Path('outputs')\n",
    "csv_paths = {\n",
    "    'BITCOIN-DEV-LEGACY': str(sorted(base_dir.glob('bitcoindev-legacy_*/data/*_date_analysis.csv'))[-1]),\n",
    "    'BITCOIN-DEV': str(sorted(base_dir.glob('bitcoindev_*/data/*_date_analysis.csv'))[-1]),\n",
    "    'LIGHTNING-DEV': str(sorted(base_dir.glob('lightning-dev_*/data/*_date_analysis.csv'))[-1]),\n",
    "    'DELVING-BITCOIN': str(sorted(base_dir.glob('delvingbitcoin_*/data/*_date_analysis.csv'))[-1])\n",
    "}\n",
    "\n",
    "\n",
    "# Create comparison directory\n",
    "comparison_dir = Path('outputs/source_comparisons')\n",
    "comparison_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Load and combine data\n",
    "combined_df = load_source_data(csv_paths)\n",
    "\n",
    "# Generate comparison heatmap\n",
    "generate_aggregate_daily_heatmap(\n",
    "    combined_df,\n",
    "    comparison_dir / f'source_comparison_heatmap_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.png'\n",
    ")\n",
    "\n",
    "print(f\"Multi-source comparison completed. Files saved in {comparison_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
